{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOEjquXlvW+PTEdG1Xu2CqS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Praveengovianalytics/10MinAIAgentChallenge/blob/main/Summarizar_Agent_with_MCP_Day1_Challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🤖 Day 1 Challenge – RAG Summarizer Agent with MCP Integration\n",
        "\n",
        "Welcome to Day 1 of the **#10MinAIAgentChallenge**!  \n",
        "Today, you’ll build an intelligent AI agent that summarizes content across various document formats using RAG (Retrieval-Augmented Generation) and exposes it as a FastAPI + MCP service.\n",
        "\n",
        "---\n",
        "\n",
        "## 🧩 Problem Statement\n",
        "\n",
        "Organizations often deal with a large volume of documents in formats like PDF, Word, PowerPoint, TXT, and CSV. Manually extracting summaries from them is time-consuming.\n",
        "\n",
        "**Your challenge:**  \n",
        "Build an AI agent that:\n",
        "- Loads unstructured documents\n",
        "- Chunks and embeds their content\n",
        "- Indexes them using FAISS\n",
        "- Responds to summarization queries via a FastAPI + MCP service\n",
        "- Uses OpenAI Agent SDK to interact in natural language\n",
        "\n",
        "---\n",
        "\n",
        "## 🛠️ What You'll Build\n",
        "\n",
        "✅ A document loader that handles `.pdf`, `.docx`, `.pptx`, `.txt`, and `.csv`  \n",
        "✅ A chunking logic for RAG-friendly text input  \n",
        "✅ Embedding using OpenAI’s `text-embedding-ada-002`  \n",
        "✅ A FAISS-based vector store  \n",
        "✅ A FastAPI service for RAG summarization  \n",
        "✅ An OpenAI Agent that queries the API and returns a concise summary\n",
        "\n",
        "---\n",
        "\n",
        "## 🚀 Workflow Summary\n",
        "\n",
        "```mermaid\n",
        "graph LR\n",
        "A[📄 Multi-format Docs] --> B[🧩 Chunk + Embed]\n",
        "B --> C[🗃️ FAISS Index]\n",
        "C --> D[🔍 RAG Inference Engine]\n",
        "D --> E[🧪 FastAPI + MCP Service]\n",
        "E --> F[🤖 OpenAI Agent Query]\n",
        "F --> G[📝 Final Summary Output]\n"
      ],
      "metadata": {
        "id": "Tfn45-PGfEgO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi fastapi_mcp uvicorn mcp-proxy phoenix-ai nest-asyncio -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcKobPK_meKU",
        "outputId": "56f65f74-dac4-4f34-e63e-7b2e6b5041f8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/236.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m235.5/236.0 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.0/236.0 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup sample data"
      ],
      "metadata": {
        "id": "KV8RrAGNnk2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx python-pptx -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_BwlPPmnws4",
        "outputId": "b1a0074f-83ba-46e7-af2e-02f586ba4110"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/472.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.6/472.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/172.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.3/172.3 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from typing import List\n",
        "from PyPDF2 import PdfReader\n",
        "from docx import Document as DocxDocument\n",
        "from pptx import Presentation\n",
        "from langchain_community.document_loaders import (\n",
        "    TextLoader, PyPDFLoader, UnstructuredWordDocumentLoader,\n",
        "    UnstructuredPowerPointLoader, UnstructuredExcelLoader\n",
        ")\n",
        "\n",
        "def ensure_folder_exists(folder_path: str):\n",
        "    os.makedirs(folder_path, exist_ok=True)\n",
        "\n",
        "def _read_pdf(file_path: str) -> str:\n",
        "    reader = PdfReader(file_path)\n",
        "    return \"\".join(page.extract_text() or \"\" for page in reader.pages)\n",
        "\n",
        "def _split_text(text: str, max_chars: int = 1000, overlap: int = 100) -> List[str]:\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(text):\n",
        "        end = min(start + max_chars, len(text))\n",
        "        if end < len(text):\n",
        "            while end > start and text[end] not in ' \\n\\t':\n",
        "                end -= 1\n",
        "            if end == start:\n",
        "                end = start + max_chars\n",
        "        chunks.append(text[start:end])\n",
        "        start = end - overlap if end - overlap > start else start + max_chars\n",
        "    return chunks\n",
        "\n",
        "def _read_docx(file_path: str) -> str:\n",
        "    doc = DocxDocument(file_path)\n",
        "    return \"\\n\".join(paragraph.text for paragraph in doc.paragraphs)\n",
        "\n",
        "def _read_pptx(file_path: str) -> str:\n",
        "    prs = Presentation(file_path)\n",
        "    lines = []\n",
        "    for slide in prs.slides:\n",
        "        for shape in slide.shapes:\n",
        "            if hasattr(shape, \"text\") and shape.text:\n",
        "                lines.append(shape.text)\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def load_and_process_single_document(folder_path: str, filename: str) -> pd.DataFrame:\n",
        "    ensure_folder_exists(folder_path)\n",
        "    file_path = os.path.join(folder_path, filename)\n",
        "    if not os.path.exists(file_path):\n",
        "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
        "\n",
        "    ext = filename.lower().rsplit(\".\", 1)[-1]\n",
        "    try:\n",
        "        if ext == \"pdf\":\n",
        "            text = _read_pdf(file_path)\n",
        "        elif ext == \"txt\":\n",
        "            text = open(file_path, \"r\", encoding=\"utf-8\").read()\n",
        "        elif ext == \"csv\":\n",
        "            df = pd.read_csv(file_path)\n",
        "            text = \"\\n\".join(\" | \".join(str(v) for v in row if pd.notna(v))\n",
        "                             for _, row in df.iterrows())\n",
        "        elif ext == \"docx\":\n",
        "            text = _read_docx(file_path)\n",
        "        elif ext == \"pptx\":\n",
        "            text = _read_pptx(file_path)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported type: .{ext}\")\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Error reading {filename}: {e}\")\n",
        "\n",
        "    chunks = _split_text(text)\n",
        "    return pd.DataFrame({\n",
        "        \"filename\": [filename] * len(chunks),\n",
        "        \"chunk_id\": list(range(len(chunks))),\n",
        "        \"content\": chunks\n",
        "    })\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Assume your functions are in document_loader.py\n",
        "#from document_loader import load_and_process_single_document, load_documents_to_dataframe, ensure_folder_exists\n",
        "\n",
        "def setup_example_files(folder: str):\n",
        "    ensure_folder_exists(folder)\n",
        "    # Create sample .txt\n",
        "    with open(os.path.join(folder, \"hello.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"Hello world! This is a test document to demonstrate chunking logic.\")\n",
        "    # Create sample .docx\n",
        "    from docx import Document\n",
        "    doc = Document()\n",
        "    doc.add_paragraph(\"This is a test Word document.\\n\" * 5)\n",
        "    doc.save(os.path.join(folder, \"sample.docx\"))\n",
        "    # Create sample .pptx\n",
        "    from pptx import Presentation\n",
        "    prs = Presentation()\n",
        "    slide = prs.slides.add_slide(prs.slide_layouts[5])  # blank layout\n",
        "    tx_box = slide.shapes.add_textbox(left=100, top=100, width=400, height=100)\n",
        "    tx = tx_box.text_frame\n",
        "    tx.text = \"Slide 1: Hello from PPTX!\"\n",
        "    prs.save(os.path.join(folder, \"presentation.pptx\"))\n",
        "\n",
        "def main():\n",
        "    folder = \"data\"\n",
        "    setup_example_files(folder)\n",
        "\n",
        "    # Process a single doc to see results\n",
        "    df_single = load_and_process_single_document(folder, \"sample.docx\")\n",
        "    print(\"Single DOCX:\", df_single)\n",
        "\n",
        "    # Load and process all supported documents\n",
        "    #df_all = load_documents_to_dataframe(folder)\n",
        "    print(\"All loaded documents:\")\n",
        "    #print(df_all.head(), \"\\nTotal records:\", len(df_all))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PK50zz8-ncI2",
        "outputId": "9f3ef71d-9ccd-476c-a945-78b08422d417"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single DOCX:       filename  chunk_id                                            content\n",
            "0  sample.docx         0  This is a test Word document.\\nThis is a test ...\n",
            "1  sample.docx         1  document.\\nThis is a test Word document.\\nThis...\n",
            "All loaded documents:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vector Pipeline"
      ],
      "metadata": {
        "id": "vAZkL1uDoCdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from phoenix_ai.loaders import load_and_process_single_document,load_documents_to_dataframe\n",
        "from phoenix_ai.vector_embedding_pipeline import VectorEmbedding\n",
        "from phoenix_ai.utils import GenAIEmbeddingClient ,GenAIChatClient\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "# Step 1: Load documents\n",
        "df = load_documents_to_dataframe(folder_path=\"data/\")\n",
        "\n",
        "# Step 2: Setup embedding client\n",
        "api_key = userdata.get('openai_api_key')  # For testing, you may replace with actual key\n",
        "embedding_model = \"text-embedding-ada-002\"\n",
        "\n",
        "embedding_client = GenAIEmbeddingClient(\n",
        "    provider=\"openai\",\n",
        "    model=embedding_model,\n",
        "    api_key=api_key\n",
        ")\n",
        "\n",
        "# Step 3: Create 'index' folder if it doesn't exist\n",
        "index_dir = \"index\"\n",
        "os.makedirs(index_dir, exist_ok=True)\n",
        "\n",
        "# Step 4: Generate FAISS index\n",
        "vector = VectorEmbedding(embedding_client, chunk_size=500, overlap=50)\n",
        "index_path, chunks = vector.generate_index(\n",
        "    df=df,\n",
        "    text_column=\"content\",\n",
        "    index_path=os.path.join(index_dir, \"policy.index\")\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31AEjFfGn9wp",
        "outputId": "754263a0-c7dc-4c97-e04d-219b8b07f5e1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📘 Loading with LangChain loader: presentation.pptx\n",
            "📘 Loading with LangChain loader: sample.docx\n",
            "📘 Loading with LangChain loader: hello.txt\n",
            "FAISS index saved with 1 chunks at index/policy.index\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set up Summarizer Service ( MCP )"
      ],
      "metadata": {
        "id": "Gk97SzC9favX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "from fastapi import FastAPI, HTTPException, Query\n",
        "from fastapi_mcp import FastApiMCP\n",
        "import uvicorn\n",
        "import threading\n",
        "from phoenix_ai.rag_inference import RAGInferencer\n",
        "from phoenix_ai.config_param import Param\n",
        "from phoenix_ai.utils import GenAIEmbeddingClient, GenAIChatClient\n",
        "from google.colab import userdata  # For Colab use\n",
        "\n",
        "# Enable nested event loop (Colab/Jupyter-safe)\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Initialize FastAPI app\n",
        "app = FastAPI(title=\"RAG Summarizer API\")\n",
        "\n",
        "# === Setup RAG Components ===\n",
        "api_key = userdata.get('openai_api_key')  # Or hardcode for local: api_key = \"sk-...\"\n",
        "\n",
        "chat_model = \"gpt-4o\"\n",
        "embedding_model = \"text-embedding-ada-002\"\n",
        "\n",
        "embedding_client = GenAIEmbeddingClient(\n",
        "    provider=\"openai\",\n",
        "    model=embedding_model,\n",
        "    api_key=api_key\n",
        ")\n",
        "\n",
        "chat_client = GenAIChatClient(\n",
        "    provider=\"openai\",\n",
        "    model=chat_model,\n",
        "    api_key=api_key\n",
        ")\n",
        "\n",
        "rag = RAGInferencer(embedding_client, chat_client)\n",
        "\n",
        "# === API Endpoint ===\n",
        "@app.get(\"/rag/summarize\")\n",
        "async def summarize(\n",
        "    question: str = Query(..., description=\"Your question or topic to summarize\"),\n",
        "    mode: str = Query(\"standard\", description=\"RAG mode to use (e.g., 'standard')\"),\n",
        "    top_k: int = Query(5, description=\"Top K documents to retrieve\")\n",
        "):\n",
        "    \"\"\"\n",
        "    RAG-based summarization using FAISS index\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = rag.infer(\n",
        "            system_prompt=Param.get_rag_prompt(),\n",
        "            index_path=\"index/policy.index\",\n",
        "            question=question,\n",
        "            mode=mode,\n",
        "            top_k=top_k\n",
        "        )\n",
        "\n",
        "        # Debug print\n",
        "        print(\"🧠 RAG Inference Output:\\n\", df)\n",
        "\n",
        "        # Check for 'response' column and last row\n",
        "        if \"response\" not in df.columns or df.empty:\n",
        "            raise ValueError(\"No valid response found in RAG output.\")\n",
        "\n",
        "        return {\n",
        "            \"question\": question,\n",
        "            \"answer\": df.iloc[-1].get(\"response\", \"No answer generated.\"),\n",
        "            \"sources\": df.to_dict(orient=\"records\")\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=f\"RAG summarization failed: {str(e)}\")\n",
        "\n",
        "# === MCP Mount ===\n",
        "mcp = FastApiMCP(\n",
        "    app,\n",
        "    name=\"RAG Summarizer API\",\n",
        "    description=\"MCP API for answering questions using RAG over FAISS index\"\n",
        ")\n",
        "mcp.mount()\n",
        "\n",
        "# === Launch Uvicorn in background thread (for Colab)\n",
        "def run_server():\n",
        "    print(\"🚀 Starting RAG Summarizer API at http://localhost:8003\")\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8003)\n",
        "\n",
        "thread = threading.Thread(target=run_server)\n",
        "thread.start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtaMZxcSik9s",
        "outputId": "05e700a5-7aa7-49e5-a77e-c2513a6c20b1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting RAG Summarizer API at http://localhost:8003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl \"http://localhost:8003/rag/summarize?question=20the%20general%20summary%3F\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYy_8VFofEKQ",
        "outputId": "4d94b642-935e-4313-d3fc-0e1294c47092"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG Answer:\n",
            " The provided slides contain repetitive text from a PowerPoint presentation that describes a test Word document to demonstrate chunking logic. Each slide repeatedly states, \"Hello from PPTX! This is a test Word document. Hello world! This is a test document to demonstrate chunking logic.\"\n",
            "🧠 RAG Inference Output:\n",
            "                                       retrieved_docs                question  \\\n",
            "0  [Slide 1: Hello from PPTX!\\nThis is a test Wor...  20the general summary?   \n",
            "\n",
            "                                              answer  \n",
            "0  The provided slides contain repetitive text fr...  \n",
            "INFO:     127.0.0.1:59632 - \"GET /rag/summarize?question=20the%20general%20summary%3F HTTP/1.1\" 500 Internal Server Error\n",
            "{\"detail\":\"RAG summarization failed: No valid response found in RAG output.\"}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Agent"
      ],
      "metadata": {
        "id": "Ipj4YJsmfm6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai-agents requests -q"
      ],
      "metadata": {
        "id": "_4P5ARIyfEHc"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, requests\n",
        "from agents import Agent, Runner, function_tool\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('openai_api_key')"
      ],
      "metadata": {
        "id": "IawQRb7wfEE0"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@function_tool\n",
        "def rag_summarizer(question: str) -> str:\n",
        "    \"\"\"\n",
        "    Query the RAG summarizer FastAPI service.\n",
        "    Args:\n",
        "        question: natural language query (e.g. \"overall summary\")\n",
        "    Returns:\n",
        "        A concise summary response.\n",
        "    \"\"\"\n",
        "    resp = requests.get(\"http://localhost:8003/rag/summarize\", params={\"question\": question})\n",
        "    resp.raise_for_status()\n",
        "    return resp.json().get(\"answer\", \"No summary.\")"
      ],
      "metadata": {
        "id": "N5SK0FFBfECJ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent(\n",
        "    name=\"RAG Agent\",\n",
        "    instructions=\"Use the RAG summarizer tool to answer user questions.\",\n",
        "    tools=[rag_summarizer],\n",
        "    # Optionally specify model, e.g. model=\"gpt-4o\"\n",
        ")"
      ],
      "metadata": {
        "id": "Uh4YdwZyrsPR"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from agents import Runner\n",
        "\n",
        "result = Runner.run_sync(agent, \"Please summarize the documents\")\n",
        "print(result.final_output)"
      ],
      "metadata": {
        "id": "WV0rrj_Hrvu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ylej9xxjry2S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}